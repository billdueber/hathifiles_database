#!/usr/bin/env ruby

# Updates the hf family of database tables with a full monthly hathifile.
# Tries to avoid disruption and thrashing by computing a delta using the
# current monthly hathifile and the state of the database and diffing
# two derivative files (database dumps rather than hathifiles) based on
# the current hf table and the hathifile to be inserted.

# By sorting and comparing these dumps we arrive at an "updates" file
# and a "deletes" file which are submitted as the `delta` parameter to
# `HathifilesDatabase::DB::Connection#update_from_file`

$:.unshift "../lib"

require "dotenv"
require "hathifiles_database"
require "hathifiles_database/constants"
require "hathifiles_database/delta"
require "hathifiles_database/dumper"
require "pathname"
require "tmpdir"

def logger
  HathifilesDatabase::Constants::LOGGER
end

def run_system_command(cmd)
  logger.info cmd
  system(cmd, exception: true)
end

envfile = Pathname.new(__dir__).parent + ".env"
Dotenv.load(envfile)

# Use the monthly hathifile from command line
hathifile = ARGV[0]
# Fall back to the most recent monthly hathifile if not specified
if hathifile.nil?
  hathifile = Dir.glob(File.join(ENV["HATHIFILES_DIR"], "hathi_full*")).max
end

connection = HathifilesDatabase.new(ENV["HATHIFILES_MYSQL_CONNECTION"])
dumper = HathifilesDatabase::Dumper.new connection

# Do all operations in temporary directory so intermediate files get cleaned up
Dir.mktmpdir do |tempdir|
  logger.info "dumping new database values from #{hathifile} to #{tempdir}"
  dump_file_paths = dumper.dump_from_file(hathifile: hathifile, output_directory: tempdir)
  new_dump = dump_file_paths[:hf].to_s
  new_dump_sorted = new_dump + ".sorted"
  current_dump = File.join(tempdir, "hf_current.txt")
  current_dump_sorted = File.join(tempdir, "hf_current.txt.sorted")

  logger.info "dumping existing database table to #{current_dump}"
  dumper.dump output_file: current_dump

  # Lists of added/modified and deleted HTIDs, one per line.
  changed_htids_path = File.join(tempdir, Pathname.new(hathifile).basename.to_s) + ".changed"
  deleted_htids_path = Pathname.new(hathifile).realdirpath.to_s + ".deleted"

  run_system_command "sort #{current_dump} > #{current_dump_sorted}"
  run_system_command "sort #{new_dump} > #{new_dump_sorted}"
  comm_cmd = "comm -13 #{current_dump_sorted} #{new_dump_sorted} | cut -f 1 > #{changed_htids_path}"
  run_system_command comm_cmd
  comm_cmd = "bash -c 'comm -23 <(cut -f 1 #{current_dump_sorted} | sort) <(cut -f 1 #{new_dump_sorted} | sort) > #{deleted_htids_path}'"
  run_system_command comm_cmd

  delta = HathifilesDatabase::Delta.new(updates_file: changed_htids_path, deletes_file: deleted_htids_path)
  logger.info "applying database delta with #{delta.updates.count} updates and #{delta.deletes.count} deletes"
  connection.update_from_file(hathifile, delta: delta)
end
