$:.unshift "../lib"

require "dotenv"
require "hathifiles_database"
require "hathifiles_database/constants"
require "hathifiles_database/delta"
require "hathifiles_database/dumper"
require "pathname"
require "tmpdir"

def logger
  HathifilesDatabase::Constants::LOGGER
end

def run_system_command(cmd)
  logger.info cmd
  system(cmd, exception: true)
end



envfile = Pathname.new(__dir__).parent + '.env'
Dotenv.load(envfile)

filename = ARGV[0]
changed_htids_path = Pathname.new(filename).realdirpath.to_s + ".changed"
deleted_htids_path = Pathname.new(filename).realdirpath.to_s + ".deleted"

connection = HathifilesDatabase.new(ENV['HATHIFILES_MYSQL_CONNECTION'])
dumper = HathifilesDatabase::Dumper.new connection

Dir.mktmpdir do |tempdir|
  logger.info "dumping new database values from #{filename} to #{tempdir}"
  dump_file_paths = dumper.dump_from_file(hathifile: filename, output_directory: tempdir)
  new_dump = dump_file_paths[:hf].to_s
  new_dump_sorted = new_dump + ".sorted"
  current_dump = File.join(tempdir, "hf_current.txt")
  current_dump_sorted = File.join(tempdir, "hf_current.txt.sorted")

  logger.info "dumping existing database table to #{current_dump}"

  dumper.dump output_file: current_dump

  run_system_command "sort #{current_dump} > #{current_dump_sorted}"
  run_system_command "sort #{new_dump} > #{new_dump_sorted}"
  comm_cmd = "comm -13 #{current_dump_sorted} #{new_dump_sorted} | cut -f 1 > #{changed_htids_path}"
  run_system_command comm_cmd
  comm_cmd = "bash -c 'comm -23 <(cut -f 1 #{current_dump_sorted} | sort) <(cut -f 1 #{new_dump_sorted} | sort) > #{deleted_htids_path}'"
  run_system_command comm_cmd
end

delta = HathifilesDatabase::Delta.new(updates_file: changed_htids_path, deletes_file: deleted_htids_path)
logger.info "database delta with #{delta.updates.count} updates and #{delta.deletes.count} deletes"

connection.update_from_file(filename, delta: delta)
